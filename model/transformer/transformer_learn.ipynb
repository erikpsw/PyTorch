{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.3116, 0.4082, 0.3891],\n",
       "          [0.6148, 0.3447, 0.5520],\n",
       "          [0.9656, 0.9069, 0.3881]],\n",
       " \n",
       "         [[0.5146, 0.1651, 0.6395],\n",
       "          [0.2647, 0.2229, 0.2476],\n",
       "          [0.6552, 0.2878, 0.7888]]]),\n",
       " tensor([[[0.3116, 0.6148, 0.9656],\n",
       "          [0.4082, 0.3447, 0.9069],\n",
       "          [0.3891, 0.5520, 0.3881]],\n",
       " \n",
       "         [[0.5146, 0.2647, 0.6552],\n",
       "          [0.1651, 0.2229, 0.2878],\n",
       "          [0.6395, 0.2476, 0.7888]]]),\n",
       " tensor([[[0.4152, 0.5471, 0.8221],\n",
       "          [0.5471, 0.8015, 1.1204],\n",
       "          [0.8221, 1.1204, 1.9054]],\n",
       " \n",
       "         [[0.7011, 0.3314, 0.8891],\n",
       "          [0.3314, 0.1811, 0.4329],\n",
       "          [0.8891, 0.4329, 1.1343]]]))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import math\n",
    "import copy\n",
    "\n",
    "K=torch.rand(2,3,3)\n",
    "K,K.transpose(-2, -1),torch.matmul(K, K.transpose(-2, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.8392, -0.2234, -1.6956],\n",
       "         [ 0.5203, -1.0467, -0.5132],\n",
       "         [ 0.5145,  1.5801, -0.4507]],\n",
       "\n",
       "        [[ 1.9833, -1.1405, -0.2046],\n",
       "         [ 1.9833, -1.1405, -0.2046],\n",
       "         [ 0.4353,  0.7404, -0.9633]]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_vocab_size=5 # 三个词\n",
    "max_seq_length=3 # 三个词语\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# 输入数据\n",
    "src_data = [['你', '是', '谁'], ['谢', '谢']]\n",
    "\n",
    "# 将每个序列转换为索引表示\n",
    "vocab = {'<PAD>':0,'你': 1, '是': 2, '谁': 3, '谢': 4}\n",
    "src_data_idx = [[vocab[word] for word in seq] for seq in src_data]\n",
    "\n",
    "# 转换为 PyTorch 的 Tensor\n",
    "src_tensors = [torch.tensor(seq) for seq in src_data_idx]\n",
    "\n",
    "# 进行填充\n",
    "padded_src_data = pad_sequence(src_tensors, batch_first=True, padding_value=vocab['<PAD>'])\n",
    "\n",
    "eb=nn.Embedding(src_vocab_size,3)\n",
    "tgt_data=eb(padded_src_data)\n",
    "\n",
    "tgt_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1, 2, 3],\n",
       "         [4, 4, 0]]),\n",
       " tensor([[[ True, False, False],\n",
       "          [ True,  True, False],\n",
       "          [ True,  True,  True]]]),\n",
       " tensor([[[[ True],\n",
       "           [ True],\n",
       "           [ True]]],\n",
       " \n",
       " \n",
       "         [[[ True],\n",
       "           [ True],\n",
       "           [False]]]]),\n",
       " tensor([[[[ True, False, False],\n",
       "           [ True,  True, False],\n",
       "           [ True,  True,  True]]],\n",
       " \n",
       " \n",
       "         [[[ True, False, False],\n",
       "           [ True,  True, False],\n",
       "           [False, False, False]]]]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#src(batch_size, seq_length)\n",
    "tgt=padded_src_data\n",
    "seq_length=tgt.size(1)\n",
    "tgt_tmp=(tgt!=0).unsqueeze(1).unsqueeze(3)\n",
    "nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool() #将主对角线及其以上的元素设为零，主对角线以下的元素设为 1\n",
    "mask=tgt_tmp&nopeak_mask\n",
    "tgt,nopeak_mask,tgt_tmp,mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 3.6293e+00, -1.0000e+09, -1.0000e+09],\n",
       "          [ 6.6746e-01,  1.6296e+00, -1.0000e+09],\n",
       "          [-2.0676e-02, -1.1548e+00,  2.9645e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 5.2761e+00, -1.0000e+09, -1.0000e+09],\n",
       "          [ 5.2761e+00,  5.2761e+00, -1.0000e+09],\n",
       "          [-1.0000e+09, -1.0000e+09, -1.0000e+09]]]],\n",
       "       grad_fn=<MaskedFillBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=tgt_data\n",
    "batch_size, seq_length, d_model = x.size() # 加一个头\n",
    "tgt_data=x.view(batch_size, seq_length, 1, d_model).transpose(1, 2)\n",
    "attn_scores = torch.matmul(tgt_data, tgt_data.transpose(-2, -1))\n",
    "attn_scores=attn_scores.masked_fill(mask == 0, -1e9)\n",
    "attn_scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
